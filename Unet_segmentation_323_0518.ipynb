{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os, glob\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import tensorflow as tf\r\n",
    "from PIL import Image\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "from keras.models import *\r\n",
    "from keras.layers import *\r\n",
    "# from keras.layers import Input, merge, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Cropping2D, concatenate, Activation,Conv2DTranspose, add\r\n",
    "from keras.layers import BatchNormalization\r\n",
    "from keras.optimizers import *\r\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, History, ReduceLROnPlateau\r\n",
    "from keras.callbacks import EarlyStopping\r\n",
    "from keras.utils.generic_utils import get_custom_objects\r\n",
    "\r\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "from sklearn.metrics import confusion_matrix\r\n",
    "\r\n",
    "from keras.utils import multi_gpu_model\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.model_selection import StratifiedKFold\r\n",
    "# from keras.utils.training_utils import multi_gpu_model\r\n",
    "\r\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\"\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "epsilon = K.epsilon()\r\n",
    "gamma = 5\r\n",
    "alpha = 0.6\r\n",
    "beta = 0.6\r\n",
    "\r\n",
    "def recall(y_target, y_pred):\r\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\r\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1))\r\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) \r\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn)\r\n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\r\n",
    "\r\n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\r\n",
    "    return recall\r\n",
    "\r\n",
    "def precision(y_target, y_pred):\r\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1))\r\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1))\r\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \r\n",
    "    count_true_positive_false_positive = K.sum(y_pred_yn)\r\n",
    "\r\n",
    "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\r\n",
    "    return precision\r\n",
    "\r\n",
    "def dice_coef(y_true, y_pred, smooth=0.001):\r\n",
    "    y_true_f = K.flatten(y_true)\r\n",
    "    y_pred_f = K.flatten(y_pred)\r\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\r\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\r\n",
    "\r\n",
    "def iou_coef(y_true, y_pred, smooth=1):\r\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])\r\n",
    "    union = K.sum(y_true,[1,2,3])+K.sum(y_pred,[1,2,3])-intersection\r\n",
    "    iou = K.mean((intersection + smooth) / (union + smooth), axis=0)\r\n",
    "    return iou\r\n",
    "\r\n",
    "def dice_coef_loss(y_true, y_pred):\r\n",
    "    return 1 - dice_coef(y_true, y_pred)\r\n",
    "\r\n",
    "\r\n",
    "get_custom_objects().update({\r\n",
    "    \r\n",
    "    'precision' : precision,\r\n",
    "    'recall' : recall,\r\n",
    "    'dice_coef' : dice_coef,\r\n",
    "    'iou_coef' : iou_coef,\r\n",
    "    'dice_coef_loss' : dice_coef_loss,\r\n",
    "        \r\n",
    "})\r\n",
    "\r\n",
    "img_size = 512\r\n",
    "depth = 4\r\n",
    "filter_sn = 32"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "inputs = Input((img_size, img_size,1), dtype='float32')\n",
    "\n",
    "def encoder_unet(inputs, filters, kernel_size= 3):\n",
    "    conv = Conv2D(filters, kernel_size, activation=None, padding='same')(inputs)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = Conv2D(filters, kernel_size, activation=None, padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "#     pool = MaxPooling2D(pool_size=(2, 2))(conv)\n",
    "    \n",
    "    return conv\n",
    "\n",
    "def decoder_unet(inputs, filters, kernel_size= 3, concate_num=depth-1):\n",
    "    up = concatenate([Conv2DTranspose(filters, (2, 2), strides=(2, 2), padding='same')(inputs), encoders[concate_num]], axis=3)\n",
    "    conv = Conv2D(filters, kernel_size, activation=None, padding='same')(up)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = Conv2D(filters, kernel_size, activation=None, padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    return conv\n",
    "\n",
    "def unet(depth=4, filter_sn=32, img_size=512):\n",
    "    global encoders\n",
    "    encoders=[]\n",
    "    \n",
    "    inputs = Input((img_size, img_size,1), dtype='float32')\n",
    "    for down in range(0,depth+1):\n",
    "#         print(down)\n",
    "        if down == 0:\n",
    "            encoder_conv = encoder_unet(inputs, filters=filter_sn, kernel_size= 3)\n",
    "            encoders.append(encoder_conv)\n",
    "            encoder_conv = MaxPooling2D(pool_size=(2, 2))(encoder_conv)\n",
    "#             print(filter_sn)\n",
    "        elif down>0 and down<depth:\n",
    "            encoder_conv = encoder_unet(encoder_conv, filters=filter_sn*(2**down), kernel_size= 3)\n",
    "            encoders.append(encoder_conv)\n",
    "            encoder_conv = MaxPooling2D(pool_size=(2, 2))(encoder_conv)\n",
    "#             print(filter_sn*(2**down))\n",
    "        else:\n",
    "            encoder_conv = encoder_unet(encoder_conv, filters=filter_sn*(2**(down-1)), kernel_size= 3)\n",
    "#             encoders.append(encoder_conv)\n",
    "#             print(filter_sn*(2**(down-1)))\n",
    "#             print(encoder_conv.shape)\n",
    "    \n",
    "\n",
    "#         print(filter_sn*(2**depth))\n",
    "        \n",
    "    for up in range(0,depth):\n",
    "        if up == 0:\n",
    "            decoder_conv= decoder_unet(encoder_conv, filters=filter_sn*(2**(down-1)), kernel_size= 3, concate_num=len(encoders)-1)\n",
    "        else:\n",
    "            decoder_conv= decoder_unet(decoder_conv, filters=filter_sn*(2**(depth-up-1)),kernel_size= 3, concate_num=depth-up-1)\n",
    "#         print('decoders:', decoder_conv.shape)\n",
    "    last_conv = Conv2D(1, (1, 1), activation='sigmoid')(decoder_conv)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=last_conv) \n",
    "    \n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model=unet(depth=4, filter_sn=32, img_size=512)\n",
    "model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def calculate_performance(pred, test_y):\n",
    "    tp=0\n",
    "    fp=0\n",
    "    tn=0\n",
    "    fn=0\n",
    "    alpha = 0.0001\n",
    "    for cm in range(len(test_y)):\n",
    "        if list(test_y)[cm]==0:\n",
    "            if pred[cm]<0.5:\n",
    "                tn+=1\n",
    "            else:\n",
    "                fn+=1\n",
    "        else:\n",
    "            if pred[cm]<0.5:\n",
    "                fp+=1\n",
    "            else:\n",
    "                tp+=1\n",
    "    print(tp, fp, tn, fn)\n",
    "    sensitivity= (tp+alpha)/(tp+fn+alpha)\n",
    "    specificity= (tn+alpha)/(tn+fp+alpha) \n",
    "    acc = (tp+tn+alpha)/(tp+fp+tn+fn+alpha)\n",
    "    prec = (tp+alpha)/(tp+fp+alpha)\n",
    "\n",
    "    return tp, fp, tn, fn, sensitivity, specificity, acc, prec"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def cal_dsc(y_true, y_pred, smooth=0.001 ):\n",
    "    dice = np.sum(y_true[y_pred==1.0])*2.0 / (np.sum(y_true) + np.sum(y_pred)+smooth)\n",
    "    return dice\n",
    "\n",
    "def confusion_matrix_jw_2(y_true, y_pred,TP=0, FP=0, TN=0,FN=0, smooth=0.001 ):\n",
    "    ravel_yt = y_true.ravel()\n",
    "    ravel_yp = y_pred.ravel()\n",
    "    for ry in range(len(ravel_yt)):\n",
    "        if ravel_yt[ry]==1 and ravel_yp[ry]==1:\n",
    "            TP+=1\n",
    "        elif ravel_yt[ry]==0 and  ravel_yp[ry]==0:\n",
    "            TN+=1\n",
    "        elif ravel_yt[ry]==0 and  ravel_yp[ry]==1:\n",
    "            FP+=1\n",
    "        else:\n",
    "            FN+=1 \n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "def performance(y_true, y_pred, smooth=0.001):\n",
    "    print(y_true.shape, y_pred.shape)\n",
    "    \n",
    "#     TP, TN, FP, FN = confusion_matrix(y_true, y_pred).ravel()\n",
    "    TP, TN, FP, FN = confusion_matrix_jw_2(y_true, y_pred)\n",
    "    accuracy = (TP + TN)/(TP+TN+FP+FN+smooth)\n",
    "    sensitivity = TP/(TP+FN+smooth)\n",
    "    specificity = TN/(TN+FP+smooth)\n",
    "    precision = TP/(TP+FP+smooth)\n",
    "    return accuracy, sensitivity, specificity, precision\n",
    "\n",
    "def binary_result(gt, results):\n",
    "    \n",
    "    Bresult = np.ndarray(results.shape, dtype=np.bool)\n",
    "    Gtruth = np.ndarray(gt.shape, dtype=np.bool)\n",
    "    # for ri in range(len(all_results)):\n",
    "    #     all_results[ri][all_results[ri]>=0.5] = 1.0\n",
    "    #     all_results[ri][all_results[ri]<0.5] = 0\n",
    "    Bresult[results>=0.5]= 1.0\n",
    "    Bresult[results<0.5]= 0\n",
    "    \n",
    "    Gtruth[gt>=0.5]= 1.0\n",
    "    Gtruth[gt<0.5]= 0\n",
    "    return Gtruth, Bresult"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_x = np.load('./new_train_img_323.npy')\n",
    "train_y = np.load('./new_train_label_323.npy')\n",
    "test_x =np.load('./new_test_img_323.npy')\n",
    "test_y = np.load('./new_test_label_323.npy')\n",
    "\n",
    "train_x = train_x/255\n",
    "train_y = train_y/255\n",
    "test_x = test_x/255\n",
    "test_y = test_y/255\n",
    "\n",
    "print(\"loading data done\") \n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_val_samples = len(train_x) // k\n",
    "print(num_val_samples)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "k=5\n",
    "savepath='./210518_Unet_001'\n",
    "model_type = 'unet_001'\n",
    "date='0518'\n",
    "os.makedirs(savepath+'/npy', exist_ok=True)\n",
    "os.makedirs(savepath+'/result', exist_ok=True)\n",
    "os.makedirs(savepath+'/model', exist_ok=True)\n",
    "num_val_samples = len(train_x) // k\n",
    "for i in range(5):\n",
    "    print('처리중인 폴드 #', i)\n",
    "    val_x = train_x[i * num_val_samples: (i + 1) * num_val_samples]  # 검증 데이터 준비: k번째 분할\n",
    "    val_y = train_y[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    partial_train_x = np.concatenate(  # 훈련 데이터 준비: 다른 분할 전체\n",
    "        [train_x[:i * num_val_samples],\n",
    "         train_x[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_y = np.concatenate(\n",
    "        [train_y[:i * num_val_samples],\n",
    "         train_y[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    \n",
    "    print(i * num_val_samples, (i + 1) * num_val_samples)\n",
    "    np.save(savepath+'/npy/{}_train_x_k{}'.format(model_type, str(i)), partial_train_x)\n",
    "    np.save(savepath+'/npy/{}_train_y_k{}'.format(model_type,str(i)), partial_train_y) \n",
    "    \n",
    "    input_img = Input(shape=(512,512,1))\n",
    "    model = unet(depth=4, filter_sn=32, img_size=512)\n",
    "#     model.summary()\n",
    "\n",
    "    data='SciRep323'\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "    option='kfold{}_batch16'.format(str(i))\n",
    "    batch_size = 16\n",
    "    monitor = 'val_loss'\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=0.001), loss=dice_coef_loss, metrics=['accuracy', recall, precision, dice_coef])\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, verbose=1, min_delta=1e-4)\n",
    "    earlystopper = EarlyStopping(monitor='val_loss',patience=30, verbose=1)\n",
    "    model_checkpoint = ModelCheckpoint(savepath+'/model/{}_{}_{}_{}.h5'.format(data,model_type,option,date), monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False)\n",
    "\n",
    "    callbacks_list = [reduce_lr, model_checkpoint, earlystopper]\n",
    "    \n",
    "    results = model.fit(partial_train_x, partial_train_y, batch_size=16, epochs=500, verbose=1,validation_data=(val_x,val_y), shuffle=False, callbacks=callbacks_list)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# i=0\n",
    "perform_index = 5\n",
    "performances = np.ndarray((k+2,perform_index), dtype=np.float32)\n",
    "indexList = np.expand_dims(np.asarray(['dice', 'accuracy', 'sensitivity', 'specificity', 'precision']), axis=0)\n",
    "rows = np.expand_dims(np.asarray([model_type,'fold0','fold1','fold2','fold3','fold4','Mean','STD']).T\n",
    "                      , axis=1)\n",
    "# for a in range(perform_index):\n",
    "#     performances[0][a] = performanceList[a]\n",
    "# test_x= np.load(savepath+'/npy/test_x_fold{}.npy'.format(str(i)))\n",
    "# test_y= np.load(savepath+'/npy/test_y_fold{}.npy'.format(str(i)))\n",
    "for i in range(k):\n",
    "    print(i)\n",
    "    option='kfold{}_batch16'.format(str(i))\n",
    "    test_model = load_model(savepath+'/model/{}_{}_{}_{}.h5'.format(data,model_type,option,date))\n",
    "    \n",
    "    _loss, _acc, _precision, _recall,_dice_coef = test_model.evaluate(test_x, test_y, batch_size=8, verbose=1)\n",
    "    print('loss: {:.3f}, accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, dice_coef:{:.3f}'.format(_loss, _acc*100, _precision*100, _recall*100, _dice_coef*100))\n",
    "    \n",
    "    test_result= test_model.predict(test_x, batch_size=64, verbose=1)\n",
    "    np.save(savepath+'/npy/test_result_fold{}.npy'.format(str(i)), test_result)\n",
    "    \n",
    "    Bresult, Gresult = binary_result(test_result, test_y)\n",
    "    dice = cal_dsc(Gresult, Bresult)\n",
    "    accuracy, sensitivity, specificity, precision = performance(Gresult, Bresult)\n",
    "    performanceList =[dice, accuracy, sensitivity, specificity, precision]\n",
    "    print(performanceList)\n",
    "    for a in range(perform_index):\n",
    "        performances[i][a] = performanceList[a]\n",
    "performances[k] = np.mean(performances[:k], axis=0)\n",
    "performances[k+1] = np.std(performances[:k], axis=0)\n",
    "print(performances.shape, indexList.shape)\n",
    "alls = np.concatenate((indexList, performances), axis=0)\n",
    "np.save(savepath+'/npy/{}_performanceList_meanNstd.npy'.format(model_type), performances)\n",
    "perform = np.concatenate((rows, alls), axis=1)\n",
    "all_perform = pd.DataFrame(perform)\n",
    "all_perform.to_csv(savepath+'/result/all_{}_perform.csv'.format(model_type))\n",
    "print(all_perform)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "indexList = np.expand_dims(np.asarray(['dice', 'accuracy', 'sensitivity', 'specificity', 'precision']), axis=0)\n",
    "\n",
    "performances[k] = np.mean(performances[:k], axis=0)\n",
    "performances[k+1] = np.std(performances[:k], axis=0)\n",
    "print(performances.shape, indexList.shape)\n",
    "alls = np.concatenate((indexList, performances), axis=0)\n",
    "np.save(savepath+'/npy/{}_performanceList_meanNstd.npy'.format(model_type), performances)\n",
    "perform = np.concatenate((rows, alls), axis=1)\n",
    "all_perform = pd.DataFrame(perform)\n",
    "all_perform.to_csv(savepath+'/result/all_{}_perform.csv'.format(model_type))\n",
    "print(all_perform)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "performances_r = performances[1:6]\n",
    "performanceList = ['dice', 'accuracy', 'sensitivity', 'specificity', 'precision']\n",
    "\n",
    "stds = np.std(performances_r, axis=0)\n",
    "means = np.mean(performances_r, axis=0)\n",
    "expd_stds = np.expand_dims(stds, axis=0)\n",
    "expd_means = np.expand_dims(means, axis=0)\n",
    "confidences = np.concatenate((expd_means, expd_stds), axis=0)\n",
    "values = np.concatenate((performances_r, confidences), axis=0)\n",
    "rows = np.expand_dims(np.asarray([model_type,'fold0','fold1','fold2','fold3','fold4','Mean','STD']).T\n",
    "                      , axis=1)\n",
    "\n",
    "arr_perf = np.expand_dims(np.asarray(performanceList), axis=0)\n",
    "\n",
    "alls = np.concatenate((arr_perf, values), axis=0)\n",
    "print(rows.shape, alls.shape)\n",
    "perform = np.concatenate((rows, alls), axis=1)\n",
    "print(perform)\n",
    "\n",
    "all_perform = pd.DataFrame(perform)\n",
    "all_perform.to_csv(savepath+'/result/all_{}_performs_only.csv'.format(model_type))\n",
    "\n",
    "# performances[k+1] = np.std(performances, axis=0)\n",
    "# performances[k+] = np.mean(performances, axis=0)\n",
    "\n",
    "# perform = np.concatenate((np.asarray(performancesList), performances), axis=0)\n",
    "np.save(savepath+'/npy/{}_performanceList_meanNstd.npy'.format(model_type), values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "performances[k+2] = np.std(performances, axis=0)\n",
    "performances[k+3] = np.mean(performances, axis=0)\n",
    "perform = np.concatenate((np.asarray(performancesList), performances), axis=0)\n",
    "np.save(savepath+'/npy/{}_performanceList.npy'.format(model_type), performances)\n",
    "all_perform = pd.DataFrame(performances)\n",
    "all_perform.to_csv(savepath+'/result/all_{}_perform.csv'.format(model_type))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(accuracy, sensitivity, specificity, precision)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(accuracy, sensitivity, specificity, precision)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(accuracy, sensitivity, specificity, precision)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(Bresult.shape, Gresult.shape)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(Bresult[0][:,:,0], cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(Gresult[0][:,:,0], cmap='gray')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(performanceList)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"loading data\")\n",
    "\n",
    "train_image = np.load('../../data/segmentation/data300/train_img300.npy')\n",
    "train_label = np.load('../../data/segmentation/data300/train_label300.npy')\n",
    "test_image =np.load('../../data/segmentation/data300/test_image_512.npy')\n",
    "test_label = np.load('../../data/segmentation/data300/test_label_512.npy')\n",
    "\n",
    "train_image = train_image/255\n",
    "train_label = train_label/255\n",
    "test_image = test_image/255\n",
    "test_label = test_label/255\n",
    "\n",
    "print(\"loading data done\")\n",
    "print(train_image.shape)\n",
    "print(train_label.shape)\n",
    "print(test_image.shape)\n",
    "print(test_label.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = model.predict(test_image, batch_size=8, verbose=1)\n",
    "np.save('./result/prob_npy/{}_{}.npy'.format(model_name,date), results)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preds_Thresh(preds_test,Thresh_value):\n",
    "    \n",
    "    preds_mask = np.ndarray((len(test_label),img_size, img_size, 1), dtype=np.float32)\n",
    "  \n",
    "    preds_mask0 = preds_test[:,:,:,0] > Thresh_value\n",
    "  \n",
    "    preds_mask[:,:,:,0] = preds_mask0*1\n",
    "    \n",
    "    return preds_mask"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "preds_mask = preds_Thresh(results,0.5)\n",
    "check_len = 5\n",
    "for toto in range(check_len):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.subplot(2,3,1)\n",
    "    plt.imshow(test_image[toto,:,:,0],cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2,3,2)\n",
    "    plt.imshow(test_label[toto,:,:,0],cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2,3,3)\n",
    "    plt.imshow(test_image[toto,:,:,0],cmap='gray')\n",
    "    plt.imshow(test_label[toto,:,:,0],cmap='Reds' ,alpha=0.6)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2,3,4)\n",
    "    plt.imshow(test_image[toto,:,:,0],cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2,3,5)\n",
    "    plt.imshow(preds_mask[toto,:,:,0],cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2,3,6)\n",
    "    plt.imshow(test_image[toto,:,:,0],cmap='gray')\n",
    "    plt.imshow(preds_mask[toto,:,:,0],cmap='Reds' ,alpha=0.6)\n",
    "    plt.axis('off')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b4b4feff2f24a0f0a34464dbe537a36fda679851528fb8735cb41fa49dffb2d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}